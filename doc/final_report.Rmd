---
title: "Identifying and Analyzing Traits Associated with High Performers"
date:  "June 29, 2020" 
author: |
  | Haoyu (Clara) Su,  Manuel Maldonado, Robert Pimentel, Thomas Pin
  | 
  | **Mentor**: Varada Kolhatkar
  | **Partner**: Glentel <br><br>

output: 
  pdf_document:
    toc: true
    number_sections: true
    df_print: kable
    fig_width: 7
    fig_height: 6
    fig_caption: true
    highlight: tango
abstract: "Here is a summary"
fontsize: 11pt
geometry: margin=1in
    
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
library(tidyverse)
library(knitr)
```


## Executive Summary
 
Glentel is a mobile phone retailer that operates 350+ wireless outlets across Canada. The primary issue they were facing at the beginning if this study was high employee churn within the first 3 month of employment, which is costly. The Master of Data Science (MDS) team was aproached to provide a solution as part of the capstone project. We proposed to solve this problem thru the implementation of natural language processing techniques. Information from employees’ resumes like education, employment history and others was extracted in order to design and generate features associated to employee traits. Such features were used as inputs to develop and train supervised machine learning models that helped the team understand and quantify the most important features that contributed to employees' high performance history. In total, 68 features were were developed (61 numeric and 7 categorical) but only 9 were selected by the finalist model as being the most informative and significant. These features are the ones Glentel should give priority to optimize their hiring process and decrease employee churn.   

## Introduction

### Data Science Problem
Glentel has requested tools to introduce evidence-based procedure to their hiring process. In addition to current COVID 19 crisis, it is critical that retail organizations have a deep understanding of their workforce. The question our team would like to help Glentel answer is **“What are the traits that are associated with employees who become high performers?”** A quantitative data analysis about their staff would be helpful to make relevant and timely decisions that could help them decrease turnover rate and optimize their workforce.

### Scientific Objective 
This project is going to identify the traits correlated to high performers through the development of a predictive model in order to help Glentel to increase productivity in sales performance and decrease turnover rate in new hires. 

### Available Data

Glentel has provided our team with unstructured data of 413 resumes and structured tabular data such as activations, compensation tier and termination reasons. Our team anticipated several data challenges at the beginning of the project that needed to be managed or mitigated somehow. First, due to no universal resume standards each resume is unique with information spread across the document. Additionally, we have documents in PDF, doc, docx, rtf and txt forms. The solution we found for this was the implementation of a resume scanner which transform the resumes into plain text, see data loading section for details. Second, some of the resumes were blank or in an unreadable form. Thus, it was decided and agreed with Glentel not to waste time looking for solutions to convert such files into usable format for loading. Third, due to Canada being a bilingual nation some of the resumes are in French. Thus, we asked our partner, Glentel, to prioritize the English ones and so the models developed during this study only consider English resumes. Finally, we highlighted to the partner the potential issues related to low observation counts in machine learning modeling studies we observed in the data set. We should recall that the population size for the studied was narrowed down to employees who worked at Glentel for at least 3 months, have a resume on file and performance data. Moreover, it was explained that a standard 80/20 split on the data was required to generate the train and test data sets in order to examine to what extent the patterns we identify in the training set generalize on the test set. So, the final training size we were left with is 289 (Figure 1). Due to a lack of resumes’ attributes in tabular form, a good portion of the MDS team’s time and resources will be utilized on data cleaning and feature extracting.
<br>

```{r plot 1, echo=FALSE, fig.cap="Figure 1. Granularity of sample size", out.width = "70%", out.height= "70%", fig.align='center'}
knitr::include_graphics("../../img/project_report/30_data_waterfall.png")
```

### High Performer Definition 

The main target for our model’s outcome is to predict employee performance in binary form: a high performer (1) or a non-high performer (0). Glentel has recommended that we use pay achievement levels as a proxy for performance. Each month, an employee can achieve a pay level between 0 and 4 depending on how many customer phone activations they attain. In Jan, Feb, Apr, May, Jul and Oct, a pay achievement level of at least 2 must be achieved and Mar, Jun, Aug, Sept, Nov and Dec a pay achievement level of at least 3 must be accomplished for an employee to gain a binary high performer flag for that month. Additionally, an employee must achieve at least an average of 75% of their monthly high-performance flags in order to reach an overall target of high performer. The first two months of employee performance data will be removed from their average as this is considered an “adjustment” period.

```{r plot 2, echo=FALSE, fig.cap="Figure 2. Example target calculation", out.width = "70%", out.height= "70%", fig.align='center'}
knitr::include_graphics("../../img/project_report/32_performance_example.png")
```

Figure 2 shows a sample calculation of a fictitious Glentel employee and how it was determined if that employee was considered high performer or non-high performer. The fictitious employee worked 11 months; the first two months will be excluded which narrows the performance window to 9 months. They received a high performance flag in 6 of those 9 months which results in a 67% high performance average and because their average is below 75% their final performance classification is non high performed. 

## Data Science Methods

### Corpus Extraction 

As mentioned above, due to the different file format of the resumes, we focused initially, in finding data loading solution for resumes with file type with the most volume, namely PDF with 228 files and Microsoft word (doc and docx) with 258 files only. To load PDF files, we used "pdf-plumber" package and for the word docx resumes, “python-docx" offered the best solution. 28 word "doc" resumes have to be first converted to "docx" format before loading them into python. The initial data loaded this way was used for preliminary EDA exercise. However, while inspecting the data, it was noticed that neither of the packages was resolving resumes were the data was split into columns, which created problems during text segmentation extraction exercise.  Therefore, it was decided to look for an alternative solution that were compatible with window systems. In the process the package called "tika" was found which did not resolve the multiple column resume issue, but it allowed us to load multiple format (PDF, doc and docx) at once. Thus, it was decided to discard pdf plumber and python-docx for simplicity. 

### Corpus Cleaning

It is worth mentioning that the way a resume gets loaded into a python is in the form of a single text string (the corpus) containing the whole resume document. Thus, for readability and manipulation it was decided to have a raw version (raw_resumme), a version where the original corpus could be read line by line to browse for information in python easily (resume_bline) and a clean processed version (clean_text) that is the standard version of machine learning processing algorithms. 

Finally, the resume data (clean and by line version) is merged with auxiliary information unique employee identifier, emplyee name, resume language, file type. An example of such file can be observed in the figure below.


```{r plot 3, echo=FALSE, fig.cap="Figure 3. Example resume data after loading", fig.align='center'}
knitr::include_graphics("../../img/project_report/33_resume_after_loading.png")
```


### EDA (Quick summary of findings)

### Topic Modeling 

### Feature Engineering 

In a nutshell, the process of feature engineering can be summarized into 3 iterative phases:  design, extracting and testing. In the design phase you develop a function to search for a phrase, word or pattern in the text. Then, you extract the information by applying such function over the whole resume text or corpus and finally test whether the function is retrieving what you want properly or whether you need to constrain the search over certain sections of the text to improve the extraction accuracy. After a couple of iterations, you might find that the function is good enough for one, two or even a handful of resumes. However, it does not take long to realize that when you are dealing with hundreds of resumes a much more efficient way to test and validate your functions is required. For us this translated into the creation of a manually extracted reference database that later became the backbone of our feature engineering activities. After careful consideration and several failed attempts of automatic information retrieval, it was decided that the reference table should not only be used for testing automatic feature functions but also to allow us to generate complex features from such a table directly. In this way we could continue advancing in the project into the machine learning pipeline phase. A summary of the information collected for the reference table can be seen below and original was saved here: **ADD LINK**

```{r plot 41, echo=FALSE, fig.cap="Table 41. Manual extraction reference table template (up to 7 jobs and 3 education fields were collected", out.width = "70%", out.height= "70%", fig.align='center'}
knitr::include_graphics("../../img/project_report/manual_extraction_template.png")
```

Going forward, we then subdivided features into "manually extracted" to refer to those generated from the reference table and "automatic extracted" to refer to those that were simple enough taht we could obtain directly from the resume corpus.

#### Manual extracted features


#### Automatic extracted features 

A Total of XX? automatic features and extracted directly from were generated 

In the figure below, an example of how to extract communication skills proxy related words is presented. In this particular case, a regex-pattern search is implemented to capture the entire list of possible combinations of root words that were considered to be in association with communication skills irrespective of user misspelling. 

Ideally, one should be able to perform  regex search under a particular segmented section of the resumme to avoid making systematic errors. For example, if instead of making the search under the skill section only, the search is done everywhere in the corpus, you might wrongly assign communications proxy skills to an employee based on the fact that the employee made reference to someone that might teach a course in comunication skills for a living. Sadly, we also found out that even when you do segmented search due to the variability of styles of the resumes, systematic erros were ocurring. 

```{r plot 5, echo=FALSE, fig.cap="Figure 4. Example of an automatic comunication skills related proxy feature", out.width = "70%", out.height= "70%", fig.align='center'}
knitr::include_graphics("../../img/project_report/auto_comm_skills_proxy.png")
```


#### Summary of generated features.

The final number features generated to feed into the machine learning model pipeline was 68. These features can be grouped into the following 11 categories depicted in the table below:

```{r plot 6, echo=FALSE, fig.cap="Table 1. Features Category Groups", out.width = "70%", out.height= "70%", fig.align='center'}
knitr::include_graphics("../../img/project_report/feature_category_groups.png")
```
 

+ **Work-experience-position:** Features in this group refers to the job title of the employee (e.g assistant manager, sales associate, etc)

+ **Academic-background:** Features in this group refer to the career path choosen by the employee during higher degree studies (e.g accounting, finance, business, art, etc).

+ **Knowledge and skills:** Features in this group try to capture individual characteristics acquired by each employee during his or hers career and is believe to distinguish them from the rest. Features from this group are often written under the skill section of a resume. 

+ **Work-experience-industry-level:** Features in this group allow us to describe and the different industry sectors (e.g Telecommunication, consumer electornics, food-serving, etc) a particular employee has previously worked for. Glentel has recognize over the years that people that has worked for the competition and has been hired tend to perform well. As such they wanted to determine if there were other particular industry sectors that show similar characteristics 

+ **Readability-spelling=grammar:**

+ **Job-tenure-industry general:**

+ **Job-count:** Features in this group attemp to quatify the number of jobs someone has worked for before joining Glentel. They can be use as proxy to estimate job hopping.

+ **Job-tenure-industry level:**

+ **internal- Glentel-profile:** Features in this group refer specifically to hired and rehired flag which Glentel was very interested to finding out if there was any assotiation to high performance in these particular groups.

+ **Work-experience-industry-level-recency:**


A detail summary of features within each category group are can be found in the following table:

```{r plot 7, echo=FALSE, fig.cap="Table 2. Engineered Features Summary", out.width = "70%", out.height= "70%", fig.align='center'}
knitr::include_graphics("../../img/project_report/features_summary.png")
```

Noticed from the above table that out of the 68 features, 7 are categorical and the rest numeric.


### Machine Learning Pipeline

The machine learning pipeline can be divided into 5 steps: data splitting, preprocessing, feature selection, hyperparameter tuning and model selection. The Feature experiment is conducted to evaluate the use of features using the finalist model. Baseline models are provided from two perspectives: dummy model is used as classifier baseline model while Bag-of-word (BOW) model and TF-IDF model are used as feature baseline models. Because of the limited dataset, 5-fold cross-validation is applied through the whole pipeline in order to use our data in a more efficient way and make a more accurate out-of-sample estimate. 

```{r, echo=FALSE, fig.cap="Figure 5. ML pipeline flow chart", out.width = "70%", out.height= "70%", fig.align='center'}
knitr::include_graphics("../../img/ml_pipeline_flow_chart.png")
```

As shown in Figure 5,  this pipeline starts by doing an 80/20 train-test split. Then data preprocessing is conducted where one-hot encoding, standardization, imputation are applied to transform, standardize values and fill in missing values. At the feature selection step, L1 regularization and recursive feature elimination (RFE) are applied with cross-validation. The reason [LassoCV](https://scikit-learn.org/stable/modules/generated/sklearn.linear_model.LassoCV.html) is applied is that L1 regularization adds a penalty to every coefficient, therefore, decreases coefficients to zero to eliminate useless and weak features. [RFECV](https://scikit-learn.org/stable/modules/generated/sklearn.feature_selection.RFECV.html) is applied is because it can recursively eliminate the weakest features to select the best number of features. The results of both methods are compared and the one that makes sense is chosen. Hyperparameter tuning for different classifiers - Logistic regression, SVM RBF, Random Forest, XGBoost, LGBM and Multi-layer perceptron - are then conducted respectively using selected features. The hyperparameter `class_weight` is tuned specifically for the imbalanced dataset.  F1 score is used as the key metric to decide the finalist model because it can balance recall and precision to help us predict as many high performers as possible and keep a reasonable precision at the same time. Accuracy can’t be trusted because the model is likely to predict all the observations to the majority to achieve good accuracy with 0 recall and 0 precision. Finally, a feature experiment is conducted to figure out if the finalist model can make good use of all groups of features that are input at the beginning of the pipeline and which group of features made the greatest contribution to the finalist model. Feature experiment is conducted by excluding one group of features each time, repeating LassoCV selection and refitting the unfitted finalist model. 

In the feature selection part, LassoCV's result is chosen because LassoCV can deal with multicollinearity issue well while RFECV can't. That's because LassoCV tune coefficients for all features together while RFECV tune coefficients for subsets of all features each time. In the model selection part, A logistic regression model with well-tuned hyperparameters is chosen as the finalist model because of its high interpretability and less overfitting issue. Logistic regression has 0.509 avg cv F1 score and 0.5 test  F1 score, which suggests that Logistic regression has less overfitting issue and can be generalized well to the testing dataset. Although SVM also has a good performance in the cross-validation part with a 0.51 avg F1 score, its F1 score drops to 0.475 in the testing dataset (Table 3). 

```{r echo=FALSE, warning=FALSE, results = "asis"}

df = read.csv("../../model/model_results/result_cv-test-merged.csv")
kable(df, col.names = c("","cv", "cv", "cv", "cv", "cv", "test", "test", "test", "test", "test"), caption = "Table 3: Cross-validation and test result")
```
Also, the error bar of logistic regression and SVM is shown in Figure 6. Error bars of them are both large which is due to our small sample size. Although the error bar of logistic regression is a little bit larger than that of SVM, we can find that the logistic regression has a greater potential to be generalized to unseen data because it can achieve a higher validation score. 

```{r, echo=FALSE, fig.cap="Figure 6. F1 score of ML classifiers (Cross validation) <br> Figure 7. Confusion matrix plot for high performers in testing dataset", fig.show="hold", out.width="50%"}
knitr::include_graphics("../../img/model_result_img/result_bar_plot_with_error_bars.png")
knitr::include_graphics("../../img/model_result_img/result_confusion_matrix_plot_test.png")
```
Finally, the finalist model is able to successfully predict around 70% high performers even though only 30% of employees in the testing dataset (Figure 7).

Feature experiment's result is shown in Table 4. The scores of the finalist model with all features is better than any of that with a subset of features. Also F1 scores dropped dramatically when work_experience_industry group is excluded, suggesting the work industry experience has a great contribution to the model. A comparison between feture baseline modes and the finalist model is also shown in table 4 to show our finalist model with well-engineered features did make improvement in high performer prediction.

```{r echo=FALSE, warning=FALSE, results = "asis"}

df_feat_exp = read.csv("../../model/model_results/result_feat_experiment.csv")
kable(df_feat_exp, caption = "Table 4. Feature experiment result")
```
## Data product and results

### Result (Dashboard figures to explain features)

Nine features are considered as important traits associated high peroformers. The top three traits are **competitor experience**, **sale customer base experience** and **trilingual flag** (Table 5).
```{r echo=FALSE, warning=FALSE, results = "asis"}

df2 = read.csv("../../model/model_results/result_finalist_weights.csv")
df2 = df2 %>% mutate(`Neg.weights` = if_else(is.na(`Neg.weights`), "", as.character(`Neg.weights`)))
kable(df2, caption = "Table 5. Weights of features used in the finalist model")
```

### Data Product 
Our group will provide Glentel with the MDS GitHub repository that contains all reports, models and scripts. The final report will consist of qualitative recommendations on the traits of high performing employees that Glentel might want to consider when hiring future employees. The models will include the models, outputs, and interpretations that the MDS team used to develop those insight in the final report. The scripts will clean and wrangle the resume data into machine learning ready spreadsheets. 


## Conclusion and Recommendation

### Conclusion
This project successful to identify the traits correlated to high performers through the development of a predictive model in order to help Glentel to increase productivity in sales performance and decrease turnover rate in new hires. Sixty-eight features are well engineered by combined manual and automatic extraction and domain knowledge. A logistic regression model with well-tuned hyperparameters is chosen as the finalist model with a 0.68 recall score. The finalist model is able to successfully predict around 70% high performers even though only 30% of employees in the testing dataset. Nine features such as competitor experience, sales customer base experience and trilingual flag are recognized as important traits correlated to high performers in new hires. A dashboard is designed to help Glentel look into each important feature and get business insights.

### Study Limitations

#### General:
- Model could be very susceptible to people copying and pasting nice text from the internet into a CV and being qualified as high performers. 
- Model does not address issues with people that might have similar CVs and hence  HP traits, but one got hired and the other rejected in the interview process.

#### Data Loading:
- Only English CVs has been considered for the study. Thus, there is a segment or region of the company is not represented (French Cvs).
- Tika package still shows problems reading CVs where information is separated into tables.

#### Data Retrieval:
Model relies on a table with information extracted manually. As such is not 100% automatic

#### Feature Engineering:
- Model is only considering soft skills like communication, leadership, team related skill in a very simple form using word proxy searches.
- Segmented search for features extracted directly from text was omitted to avoid losing observation counts.

#### Data Modeling:
- Investigate in more detail the impact of soft skills features describe above on the model.
- Study dependencies of Trilingual feature to geography.
- Due to the low metrics obtained during the study, the current model should not be use for prediction.


### Recommendation 

#### General:
- Incorporate French language CVs into the study.

#### Data Loading:
- Investigate batch converting files to simply text format and loading to  avoid miss-location of information in CVs with tables.

#### Data Retrieval:
- Complement the study with Automatic CV data extraction tool using annotating packages and Spacy.  Perhaps, even considered this to be a separate project on its own.
- Consider Implementing a web browser resume submission system if HR department  is to rerun this type of analysis on a regular basis.

#### Feature Engineering:
- Standardize the process of generating features using key words and regex patterns and list 
- Develop and engineer soft skills features like leadership, customer service, communication and sales based on group of key words by level. (eg. Low, med and high).

#### Data Modeling:
- Investigate in more detail the impact of soft skills features describe above on the model.
- Study dependencies of Trilingual feature to geography.




